{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7e2bc206",
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import awkward as ak\n",
    "import energyflow as ef\n",
    "import fastjet as fj\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import vector\n",
    "from cycler import cycler\n",
    "import uproot\n",
    "import logging\n",
    "from pathlib import Path\n",
    "from omegaconf import OmegaConf\n",
    "from collections import deque\n",
    "from pathlib import Path\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a3fc6137",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "project_root = os.path.abspath(\"..\")\n",
    "if project_root not in sys.path:\n",
    "    sys.path.append(project_root)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5460d3ca",
   "metadata": {},
   "source": [
    "### Functions to process JetClass data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "76b6c421",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read jet histories and filter the ones clustered into only one inclusive jet\n",
    "# which often result in lower pT jets (i.e. outliers)\n",
    "def read_histories(cluster_seq, pt_lower=0, pt_upper=np.inf): \n",
    "\n",
    "    histories = ak.Array(cluster_seq.unique_history_order())\n",
    "    inc_jets = cluster_seq.inclusive_jets()\n",
    "\n",
    "    # Get number of inclusive jets per reclustered jet\n",
    "    num_inc_jets = ak.num(inc_jets)\n",
    "\n",
    "    # Get the pt of the first jet\n",
    "    jet_pts = ak.firsts(inc_jets.pt)\n",
    "\n",
    "    # Filtering mask\n",
    "    mask = (num_inc_jets == 1) & (pt_lower < jet_pts) & (jet_pts < pt_upper)\n",
    "\n",
    "    # Filter histories and indices\n",
    "    filtered_histories = histories[mask]\n",
    "    filtered_indices = np.nonzero(mask)[0]\n",
    "\n",
    "    return filtered_histories, filtered_indices\n",
    "\n",
    "\n",
    "# Return list of declustering steps (pseudo particle decays) from jets\n",
    "# Utilises cluster.jets() and cluster.unique_history_order()\n",
    "def return_decays(jet_arrays, jet_hists, num_consts, dummy_momentum, return_indices=True):\n",
    "    decays = []\n",
    "    if return_indices == True:\n",
    "        for i in range(len(jet_arrays)):\n",
    "            hist = jet_hists[i]\n",
    "            n_consts = num_consts[i]\n",
    "            last_idx = hist[-1]\n",
    "\n",
    "            stack = []\n",
    "            jet_decays = []\n",
    "\n",
    "            for idx in hist:\n",
    "                # End of the cluster history reached\n",
    "                if idx == last_idx:\n",
    "                    break\n",
    "                if idx < n_consts:\n",
    "                    # Constituent, no decay\n",
    "                    stack.append(idx)\n",
    "\n",
    "                    # Inserting at beginning of the list to get reversed order\n",
    "                    # Constituent index and two placeholders, since no further decay\n",
    "                    jet_decays.insert(0, [idx, None, None])\n",
    "                else:\n",
    "                    # Merge of the two last items in the stack\n",
    "                    left = stack.pop()\n",
    "                    right = stack.pop()\n",
    "                    stack.append(idx)\n",
    "\n",
    "                    # Insert pseudojet index and two children\n",
    "                    jet_decays.insert(0, [idx, left, right])\n",
    " \n",
    "            decays.append(jet_decays)\n",
    "\n",
    "        return decays\n",
    "\n",
    "    # Return 4-momentum instead of tokens\n",
    "    else:\n",
    "        for i in range(len(jet_arrays)):\n",
    "            hist = jet_hists[i]\n",
    "            jet_array = jet_arrays[i]\n",
    "            n_consts = num_consts[i]\n",
    "\n",
    "            stack = []\n",
    "            jet_decays = []\n",
    "\n",
    "            for idx in hist:\n",
    "                # End of the cluster history reached\n",
    "                if idx == hist[-1]:\n",
    "                    break\n",
    "\n",
    "                if idx < n_consts:\n",
    "                    # Constituent, no decay\n",
    "                    stack.append(idx)\n",
    "                    # Append parent 4-momentum and two dummy values\n",
    "                    jet_decays.append([jet_array[idx], dummy_momentum, dummy_momentum])\n",
    "\n",
    "                else:\n",
    "                    # Merge of the two last items in the stack\n",
    "                    left = stack.pop()\n",
    "                    right = stack.pop()\n",
    "                    stack.append(idx)\n",
    "                    jet_decays.append([jet_array[idx], jet_array[left], jet_array[right]])\n",
    "                        \n",
    "        return decays\n",
    "    \n",
    "\n",
    "# Insert the corresponding tokens for the declustering steps of each jet    \n",
    "def tokenise_decays(jet_decays, token_arrays, codebook_size):\n",
    "    end_token = codebook_size\n",
    "    masked_decays = ak.fill_none(jet_decays, -1)\n",
    "\n",
    "    # Get the tokens to the same shape as the decays\n",
    "    masked_tokens = token_arrays[ak.local_index(token_arrays, axis=1)][ak.where(masked_decays != -1, masked_decays, 0)]\n",
    "\n",
    "    result = ak.where(masked_decays == -1, end_token, masked_tokens)\n",
    "    \n",
    "    # Add start and end row to the tokenised decays\n",
    "    # Start row: list of zeros\n",
    "    # End row: list of end tokens (codebook size + 1)\n",
    "    return ak.concatenate(\n",
    "    [\n",
    "        ak.zeros_like(result[:, :1]),\n",
    "        result + 1,\n",
    "        ak.ones_like(result[:, :1]) + end_token,\n",
    "    ],\n",
    "    axis=1,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d616793b",
   "metadata": {},
   "source": [
    "### Reading unique history order when jets are reclustered into multiple inclusive jets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "840c9847",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_histories(cluster_seq, num_jets): \n",
    "    corrected_histories = []\n",
    "\n",
    "    histories = ak.Array(cluster_seq.unique_history_order())\n",
    "    num_consts = cluster_seq.n_particles()\n",
    "    jet_arrays = cluster_seq.jets()\n",
    "    inc_jets = cluster_seq.inclusive_jets()\n",
    "\n",
    "    for i, hist in enumerate(histories[:num_jets]):\n",
    "        jet_history = hist.tolist()\n",
    "        jet_array = jet_arrays[i]\n",
    "        length_array = len(jet_array)\n",
    "\n",
    "        # Find the index at which the inclusive jet in the cluster history ends\n",
    "        num_inc_jets = len(inc_jets[i])\n",
    "        inc_jet_pt = inc_jets[i].pt\n",
    "        jet_array_pt = jet_array.pt\n",
    "        cluster_ends = []\n",
    "\n",
    "        # Account for .jets() being reindexed after clustering to fix mismatch with .unique_history_order()\n",
    "        if num_inc_jets > 1:\n",
    "            # Get indices for the inclusive jet pt from .jets_out()\n",
    "            inc_indices = [i for i, x in enumerate(jet_array_pt) if x in inc_jet_pt]\n",
    "            # Loop backwards through history\n",
    "            for cluster_step in reversed(range(len(jet_history)-1)):\n",
    "                # If index of inclusive jet is found, the one after it is a placeholder index\n",
    "                if jet_history[cluster_step] in inc_indices:\n",
    "                    # Ensure that the next element is not a constituent, but a pseudojet\n",
    "                    if jet_history[cluster_step+1] > num_consts[i]:\n",
    "                        idx_to_adjust = jet_history[cluster_step+1]\n",
    "                        # Get the index for the placeholder\n",
    "                        cluster_ends.append(cluster_step+1)\n",
    "                        # And adjust the history by subtracting 1 from every index larger than the placeholder\n",
    "                        jet_history = [x if x <= idx_to_adjust else x - 1 for x in jet_history]\n",
    "            \n",
    "            # Go through the placeholder indices and set them to the length of the whole array\n",
    "            # This means each inclusive jet gets the same ending placeholder\n",
    "            for cluster_end in cluster_ends:\n",
    "                jet_history[cluster_end] = length_array\n",
    "            \n",
    "            jet_histories = []\n",
    "            current_hist = []\n",
    "            for x in jet_history:\n",
    "                current_hist.append(x)\n",
    "                if x == length_array:\n",
    "                    jet_histories.append(current_hist)\n",
    "                    current_hist = []\n",
    "            corrected_histories.append(jet_histories)\n",
    "        else:\n",
    "            corrected_histories.append([jet_history])\n",
    "\n",
    "    return ak.Array(corrected_histories)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36173bb5",
   "metadata": {},
   "source": [
    "### Load the VQ-VAE model for tokenisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bacca88",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gabbro.models.vqvae import VQVAELightning\n",
    "\n",
    "# this checkpoint is the checkpoint from a tokenization training\n",
    "ckpt_path = \"../checkpoints/vqvae_12288_tokens/last.ckpt\"\n",
    "cfg = OmegaConf.load(Path(ckpt_path).parent / \"config.yaml\")\n",
    "pp_dict = OmegaConf.to_container(cfg.data.dataset_kwargs_common.feature_dict)\n",
    "\n",
    "pp_dict_cuts = {\n",
    "    feat_name: {\n",
    "        criterion: pp_dict[feat_name].get(criterion)\n",
    "        for criterion in [\"larger_than\", \"smaller_than\"]\n",
    "    }\n",
    "    for feat_name in pp_dict\n",
    "}\n",
    "\n",
    "pp_dict_transform = {\n",
    "    feat_name: {\n",
    "        key: value \n",
    "        for key, value in feat_settings.items() \n",
    "        if key not in [\"larger_than\", \"smaller_than\"]\n",
    "        }\n",
    "    for feat_name, feat_settings in pp_dict.items()\n",
    "}\n",
    "\n",
    "# hacky way to setup logging in jupyter\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger.info(\"Setup complete\")\n",
    "vqvae_model = VQVAELightning.load_from_checkpoint(ckpt_path)\n",
    "\n",
    "print(\"\\nModel:\")\n",
    "print(vqvae_model)\n",
    "vqvae_model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "101a8666",
   "metadata": {},
   "source": [
    "### Process dataset (extract declustering sequences + tokenise jets and insert tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a929d0dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gabbro.utils.arrays import ak_select\n",
    "\n",
    "# Define the dataset directory\n",
    "dataset_dir = \"jetclass/\"\n",
    "subdirs = [\"test_20M/\", \"train_100M/\", \"val_5M/\"]\n",
    "\n",
    "# How many files per folder & jets per file to load and process\n",
    "num_files = 3\n",
    "n_load = 10000\n",
    "\n",
    "# How many jets to process at a time, eg. 20% \n",
    "# In order to avoid running out of memory\n",
    "batch_size = n_load // 5\n",
    "\n",
    "# Dummy 4-momentum object for children of constituents\n",
    "zero_momentum = ak.zip(\n",
    "    {\"px\": 0, \"py\": 0, \"pz\": 0, \"E\": 0},\n",
    "    with_name=\"Momentum4D\",\n",
    "    behavior=vector.backends.awkward.behavior,\n",
    ")\n",
    "\n",
    "# Codebook size from the VQ-VAE\n",
    "codebook_size = np.float32(vqvae_model.model.vqlayer.num_codes)\n",
    "\n",
    "###---------------------------------------------------------###\n",
    "### Open one file at a time and extract particle 4-momentum ###\n",
    "###---------------------------------------------------------###\n",
    "\n",
    "print(f\"Number of files to process: {num_files}\")\n",
    "print(f\"Using folders {subdirs}\\n\")\n",
    "\n",
    "# Go through each file at a time\n",
    "for subdir in subdirs:\n",
    "    print(f\"Now processing files in: {dataset_dir+subdir}\")\n",
    "    files = os.listdir(dataset_dir + subdir)\n",
    "    sorted_files = sorted(files)\n",
    "    output_dir = Path(dataset_dir.replace(\"jetclass\", \"jetclass_tokenised\") + subdir)\n",
    "\n",
    "    # Create output directories\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    for filename in sorted_files[:num_files]:\n",
    "        # Optional: Record time taken per file\n",
    "        start = time.perf_counter()\n",
    "\n",
    "        # Generate the name of the parquet file\n",
    "        filename_parquet = Path(filename).name.replace(\".root\", \"_tokenised.parquet\")\n",
    "        if filename_parquet in os.listdir(output_dir):\n",
    "            print(f\"File {filename_parquet} already present, skipping file.\")\n",
    "        else:\n",
    "            print(f\"Using {n_load} jets from file {filename}\")\n",
    "            filepath = dataset_dir + subdir + filename\n",
    "\n",
    "            # Open the file and load jets\n",
    "            file = uproot.open(filepath)\n",
    "            jets = file[\"tree\"].arrays()[:n_load]\n",
    "\n",
    "            # Close the root file\n",
    "            file.close()\n",
    "\n",
    "            vector.register_awkward()\n",
    "\n",
    "            # Create 4-momentum vector\n",
    "            p4 = ak.zip(\n",
    "                {\n",
    "                    \"px\": jets[\"part_px\"],\n",
    "                    \"py\": jets[\"part_py\"],\n",
    "                    \"pz\": jets[\"part_pz\"],\n",
    "                    \"E\": jets[\"part_energy\"],\n",
    "                },\n",
    "                with_name=\"Momentum4D\",  \n",
    "                behavior=vector.backends.awkward.behavior,  \n",
    "            )\n",
    "\n",
    "            ###-----------------------------------------------###\n",
    "            ### Truncate jets to 128 particles and apply cuts ###\n",
    "            ###-----------------------------------------------###\n",
    "\n",
    "            p4 = p4[:, :128]\n",
    "\n",
    "            # For computing relative eta and phi\n",
    "            p4_jet = ak.sum(p4, axis=1)\n",
    "\n",
    "            unmasked_particles = ak.zip({\"part_pt\": p4.pt, \"part_etarel\": p4.deltaeta(p4_jet), \"part_phirel\": p4.deltaphi(p4_jet), \"mass\": p4.mass}, with_name=\"Momentum4D\")\n",
    "\n",
    "            # Apply preprocessing cuts (without applying transforms)\n",
    "            mask = ak_select(unmasked_particles, pp_dict_cuts)\n",
    "            masked_particles = p4[mask]\n",
    "\n",
    "            ###------------------------------------------------------------###\n",
    "            ### Cluster and filter out jets with more than 1 inclusive jet ###\n",
    "            ###------------------------------------------------------------###\n",
    "\n",
    "            #  Define the clustering algorithm (kt algorithm with R=0.8)\n",
    "            jet_def = fj.JetDefinition(fj.kt_algorithm, 0.8, fj.WTA_pt_scheme)\n",
    "            print(f\"Particles from {n_load} jets are being clustered with the following algorithm:\\n{jet_def}\")\n",
    "\n",
    "            cluster = fj.ClusterSequence(masked_particles[:n_load], jet_def)\n",
    "\n",
    "            # Get jets and constituents\n",
    "            jets_out = cluster.inclusive_jets()\n",
    "            consts_out = cluster.constituents()\n",
    "            num_particles = cluster.n_particles()\n",
    "            jet_structure_array = cluster.jets()\n",
    "\n",
    "            # Get histories and indices of jets between 500 and 1000 pt\n",
    "            pt_cuts = {\"pt_lower\": 500, \"pt_upper\": 1000}\n",
    "            jet_hists, jet_indices  = read_histories(cluster, pt_cuts[\"pt_lower\"], pt_cuts[\"pt_upper\"])\n",
    "\n",
    "            # Filter the jet structure array to only keep single inclusive jets\n",
    "            jets_filtered = jet_structure_array[jet_indices]\n",
    "            num_particles_filtered = num_particles[jet_indices]\n",
    "            inc_jets_filtered = jets_out[jet_indices]\n",
    "\n",
    "            ###------------------###\n",
    "            ### Jet tokenisation ###\n",
    "            ###------------------###\n",
    "\n",
    "            # Using batchwise processing\n",
    "            results = []\n",
    "            for i in range(0, len(jets_out), batch_size):\n",
    "                print(f\"Tokenising current batch: {i} - {i+batch_size}\")\n",
    "                # Get the respective inclusive jet for calculating relative eta/phi\n",
    "                p4_inc_jets = ak.firsts(inc_jets_filtered[i:i+batch_size])\n",
    "                jets_batch = jets_filtered[i:i+batch_size]\n",
    "                \n",
    "                jets_ak = ak.zip(\n",
    "                    {\n",
    "                        \"part_pt\": jets_batch.pt, \n",
    "                        \"part_etarel\": jets_batch.deltaeta(p4_inc_jets), \n",
    "                        \"part_phirel\": jets_batch.deltaphi(p4_inc_jets)\n",
    "                    }, \n",
    "                    with_name=\"Momentum4D\")\n",
    "                \n",
    "                # Tokenise jets\n",
    "                jets_tokenized = vqvae_model.tokenize_ak_array(\n",
    "                    ak_arr=jets_ak,\n",
    "                    pp_dict=pp_dict_transform,\n",
    "                    batch_size=512,\n",
    "                    pad_length=256,\n",
    "                )\n",
    "\n",
    "                results.append(jets_tokenized)\n",
    "\n",
    "            jets_tokenised = ak.concatenate(results)\n",
    "\n",
    "            ###----------------------------------------------###\n",
    "            ### Extract declustering steps and insert tokens ###\n",
    "            ###----------------------------------------------###\n",
    "\n",
    "            results = []\n",
    "            for i in range(0, len(jets_filtered), batch_size):\n",
    "                print(f\"Extracting decays and inserting tokens for current batch: {i} - {i+batch_size}\")\n",
    "                batch_jets = jets_filtered[i:i+batch_size]\n",
    "                batch_hists = jet_hists[i:i+batch_size]\n",
    "                batch_num_parts = num_particles_filtered[i:i+batch_size]\n",
    "                batch_tokens = jets_tokenised[i:i+batch_size]\n",
    "                \n",
    "                # Returns awkward arrays with the decluster sequence per jet\n",
    "                # Declustering steps are represented by triplets: [parent, left, right]\n",
    "                ak_decays = return_decays(batch_jets, batch_hists, batch_num_parts, zero_momentum)\n",
    "                \n",
    "                # Convert jets from triplet structure to 1-dimensional arrays\n",
    "                ak_decays = ak.flatten(ak_decays, axis=2)\n",
    "\n",
    "                # Replace indices with tokens\n",
    "                ak_tokens = tokenise_decays(ak_decays, batch_tokens, codebook_size)\n",
    "                results.append(ak_tokens)\n",
    "\n",
    "            ak_tokens = ak.concatenate(results)\n",
    "            print(f\"Successfully tokenised {len(ak_tokens)} jets.\")\n",
    "\n",
    "            # Release memory\n",
    "            del jets, p4, unmasked_particles, mask, masked_particles, cluster, jets_out\n",
    "            del consts_out, num_particles, jet_structure_array, jet_hists, jet_indices, jets_filtered, jets_tokenised\n",
    "            del results, batch_jets, batch_hists, batch_num_parts, batch_tokens, ak_decays, num_particles_filtered\n",
    "\n",
    "            ###-------------------------###\n",
    "            ### Store as .parquet files ###\n",
    "            ###-------------------------###\n",
    "\n",
    "            # Generate the path for saving the file\n",
    "            filename_out = output_dir / filename_parquet\n",
    "\n",
    "            # Record the time it took to process one file\n",
    "            end = time.perf_counter()\n",
    "            print(f\"File: {filename} finished processing after {end - start:.4f} seconds\")\n",
    "\n",
    "            # Save the processed data to file\n",
    "            print(f\"Saving tokenised file to {filename_out}\")\n",
    "            ak.to_parquet(ak_tokens, filename_out)\n",
    "            print(\"Saving completed.\")\n",
    "\n",
    "print(\"Done.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
