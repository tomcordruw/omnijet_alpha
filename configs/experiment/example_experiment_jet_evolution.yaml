# @package _global_

# to execute this experiment run:
# python gabbro/train.py experiment=example_experiment_jet_evolution

defaults:
  - override /data: iter_dataset_jet_evolution.yaml
  - override /model: backbone_jet_evolution.yaml
  - override /callbacks: callbacks_for_generative_training.yaml
  - override /trainer: gpu.yaml

# all parameters below will be merged with parameters from default configurations set above
# this allows you to overwrite only specified parameters

project_name: "omnijet-jet-evolution"
tags: ["generative"]

run_note: ""

seed: 1603


data:
  batch_size: 128  # NOTE: adapt the limit_train_batches accordingly
  # NOTE: the `data_dir` must also include the `model_ckpt.ckpt` file of the tokenizer
  #       and the `config.yaml` that was used for the tokenization training
  #       since those will be used during the training to reconstruct the tokens
  #       to physical space
  data_dir: ./datasets/jetclass_tokenised
  dataset_kwargs_train:
    max_n_files_per_type: 30
  dataset_kwargs_val:
    shuffle_only_once: false
  dataset_kwargs_common:
    load_only_once: true
    pad_length: 256 # Embedding dim must be same as pad length or higher
    n_files_at_once: 30
    n_jets_per_file: 5000
    feature_dict:
      # part_token_id: {}
      part_token_id_without_last: {}  # <-- this will be the input for the gen. model
      part_token_id_without_first: {} # <-- this will be the target for the gen. model
    token_id_cfg:
      remove_start_token: false
      remove_end_token: false
      shift_tokens_minus_one: false

callbacks:
  generative_callback:
    n_val_gen_jets: 10
    starting_at_epoch: 0
    batch_size_for_generation: 24
  early_stopping:
    patience: 20 # number of checks with no improvement after which training will be stopped
trainer:
  max_epochs: 3
  gradient_clip_val: 5
  log_every_n_steps: 10
  limit_train_batches: 400 # 1900 with batch size 512, to have 1M samples per epoch
  limit_val_batches: 20

# setting load_weights_from will load the weights from the given checkpoint, but start training from scratch
load_weights_from: null
load_weights_strict: false

model:
  # --- model architecture configuration ---
  model_kwargs_loaded: null
  token_dir: ${data.data_dir}
  model_kwargs:
    # ---
    return_embeddings: true
    backbone_weights_path: null
    token_weights_path: null # Can add a path here for token weights, may help with token imbalances 
    # ---
    embedding_dim: 256
    attention_dropout: 0.1
    vocab_size: 12290 # codebook size + start token + stop token, e.g. 8192 + 1 + 1 = 8194
    max_sequence_len: 256 # Embedding dim must be same as sequence length or higher
    n_GPT_blocks: 3
    n_heads: 8
  # --- optimizer configuration ---
  optimizer:
    _target_: torch.optim.AdamW
    _partial_: true
    lr: 1e-3
    weight_decay: 1e-2
  # --- learning rate scheduler ---
  scheduler:
    _target_: torch.optim.lr_scheduler.ConstantLR
    _partial_: true
    total_iters: 1

task_name: "omnijet_backbone"

logger:
  wandb:
    project: ${project_name}
    tags: ${tags}
    name: ${task_name}
  comet:
    experiment_name: null
    project_name: ${project_name}